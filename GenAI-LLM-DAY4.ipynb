{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2a645-115c-4020-a07e-c568cc9d502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL - |\n",
    "#  prompt - promt_Template \n",
    "#  memory \n",
    "#  streamlit\n",
    "#   |->FastAPI \n",
    "# built chatbot - QA\n",
    "\n",
    "prompt - building block of LangchainCode\n",
    "-----  - interact with LLM\n",
    "       - it is an iterface between taking an input from user and structring a sentence that goes\n",
    "         to the LLM\n",
    "\n",
    "PromptTemplate {  input_variable = [\"label\"], template = \"message {label}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9757caf-e6d3-4b37-a274-ad715af23468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15dc6d1c-45d4-4473-ba40-a1915d5a4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62d512fe-5c9d-4f6a-b3ac-97c7af8e181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c206ea07-b45d-4a62-a052-884e2562e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= OpenAI()\n",
    "#llm(\"Give me 3 best place to visit in india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9ab2172-56a5-41eb-bcdd-9b443f40e47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "print(PromptTemplate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbaa2a-26de-4b2c-88d2-280a829c6e52",
   "metadata": {},
   "source": [
    "promt_obj = PromptTemplate(input_var = [\"book\"],\n",
    "                           result = \"I likes to read this {book}\"\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf95eb31-2850-4dae-9051-9b894beeab45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['book'], input_types={}, partial_variables={}, template='I likes to read this {book} ')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template(\"I likes to read this {book} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80b7a24c-2b4f-4e6a-abca-808d6f58511f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read this python '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt2_obj = PromptTemplate.from_template(\"I likes to read this {book} \")\n",
    "promt2_obj.format(book=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1b2e0d2-6077-441c-b38c-3abc0b829048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read this GenAI '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt2_obj.format(book=\"GenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a8b2424-ea8c-4b84-9f65-e91a591905c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt3_obj = PromptTemplate.from_template(\"My self {name} i am from {place} iam belongs to {dept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bec1502d-7903-4be7-a762-87017a1028fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My self leo i am from pune iam belongs to sales'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt3_obj.format(name=\"leo\",place=\"pune\",dept=\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7777dcf-86ac-4375-af0e-ea996b678544",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt4_obj = (PromptTemplate.from_template(\"Tell me about {topic}\")+\", yes its nice hear\"+\"\\n thanks {name} about your talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2ab3def-5e90-481f-8301-63cc7103018a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me about water, yes its nice hear\\n thanks Paul about your talk'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt4_obj.format(topic=\"water\",name=\"Paul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ca6ffff-9445-418c-bcad-52823e4d6301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about water, yes its nice hear\n",
      " thanks Paul about your talk\n"
     ]
    }
   ],
   "source": [
    "print(promt4_obj.format(topic=\"water\",name=\"Paul\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "050f612f-30de-47b5-afb6-7745532afc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give some short story about sports in english'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt5_obj = PromptTemplate.from_template(\"Give some short story about {topic} in {language}\")\n",
    "prompt5_obj.format(topic=\"sports\",language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40ee90-9771-436a-8683-8f48caf7518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(prompt5_obj.format(topic=\"sports\",language=\"english\"),number=n,adjective=\"short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174aa71-fd43-4c75-a05c-ffd83124fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct message \n",
    "#-------------------\n",
    "# belongs to llm model - ChatPromptTemplate - OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\",\"baseline \"),\n",
    "      (\"human\",\"user inputs\"),\n",
    "      (\"ai\",\"response from LLM\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\",\"Hi your act as AI assist\"),\n",
    "      (\"human\",\"Hello \"),\n",
    "      (\"ai\",\"Hello,How can i help you?\"),\n",
    "      (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6a9d4-ee0b-43b3-a2d8-60a0d7477449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "x.from_messages( \n",
    "    [ SystemMessagePromptTemplate.from_template(\"you are AI ass...{input_var}...\")\n",
    "     HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "780b0042-9592-40b6-bcb3-5ddd6674b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9387066d-33cc-4628-b7e5-8dc3e0e573a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f753f85-b933-43b9-90f1-a953b480d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6485f1c7-6d23-40c4-9934-7db6d8837901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "622c15b1-bea2-48a7-bdd7-ed973b49e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\",\"you are a helpful AI assistant,Please respond to the Q asked\"),\n",
    "     (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d910def2-a2f7-4c3e-a386-8e3b943ad850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theeba\\AppData\\Local\\Temp\\ipykernel_16812\\577058090.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model = \"gemma:2b\")\n"
     ]
    }
   ],
   "source": [
    "## Ollama model\n",
    "llm = Ollama(model = \"gemma:2b\")\n",
    "output = StrOutputParser()\n",
    "chain = prompt|llm|output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21155215-e494-4a91-810b-f021583917ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, I'd be happy to answer that question.\\n\\n**GenAI** is a cutting-edge field that encompasses the development of **Generative Artificial Intelligence (AI)** systems that can create new content, like text, images, music, and more. These AI systems work by learning from vast amounts of data and using algorithms to identify patterns and relationships within that data.\\n\\n**Here's a more detailed breakdown:**\\n\\n* **Generative AI** focuses on creating new, original content, rather than just copying existing data.\\n* It utilizes advanced machine learning techniques, including deep learning, to analyze and understand vast amounts of data.\\n* By drawing insights from this data, genAI systems can generate realistic and diverse outputs, such as text, images, audio, and more.\\n\\n**Key characteristics of genAI include:**\\n\\n* **Creativity:** The ability to generate new and original content.\\n* **Data-driven:** The reliance on vast amounts of data for learning.\\n* **Algorithmic:** The use of algorithms and deep learning techniques to understand and generate content.\\n* **Application-oriented:** The focus on finding practical applications for genAI systems.\\n\\n**Examples of genAI technologies include:**\\n\\n* **Chatbots:** AI-powered conversational assistants that can generate natural language responses.\\n* **Content creation tools:** Platforms that allow users to generate images, videos, and other creative content.\\n* **Language models:** AI systems that can translate languages with impressive accuracy.\\n* **Music generation:** AI systems that can create music, lyrics, and even musical arrangements.\\n\\n**Overall, genAI is a rapidly evolving field with vast potential to revolutionize various industries, including:**\\n\\n* **Content creation:** Marketing, advertising, and entertainment\\n* **Drug discovery:** Pharmaceuticals and medical research\\n* **Manufacturing and design:** Product development and innovation\\n* **Education and training:** Personalized learning experiences\\n\\nI hope this explanation clarifies what genAI is. Please let me know if you have any further questions.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\":\"what is genAI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57474557-35ef-48b1-93bd-d810e9f616d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s how to write a factorial program in Python:\\n\\n```python\\n# Define the factorial function\\ndef factorial(n):\\n    \"\"\"\\n    Calculates the factorial of a non-negative integer n.\\n\\n    Args:\\n        n (int): The non-negative integer for which to calculate the factorial.\\n\\n    Returns:\\n        int: The factorial of n.\\n    \"\"\"\\n\\n    # Initialize the factorial to 1 (since 0! is defined as 1!).\\n    factorial = 1\\n\\n    # Iterate through the numbers from 2 to n.\\n    for i in range(2, n + 1):\\n        # Multiply the factorial of i by the factorial of n.\\n        factorial *= i\\n\\n    # Return the factorial of n.\\n    return factorial\\n\\n\\n# Get the input from the user\\nn = int(input(\"Enter a non-negative integer: \"))\\n\\n# Calculate and print the factorial of n\\nresult = factorial(n)\\nprint(f\"The factorial of {n} is {result}\")\\n```\\n\\n**Explanation:**\\n\\n1. The `factorial` function takes an integer `n` as input.\\n2. It initializes the `factorial` variable to 1, as 0! is defined as 1!.\\n3. It then iterates through the numbers from 2 to `n` (inclusive).\\n4. Inside the loop, it multiplies the `factorial` of each number by the `factorial` of `n`.\\n5. Finally, it returns the `factorial` of `n`.\\n6. The user is prompted to input a non-negative integer, and the program calculates and prints the factorial of that number.\\n\\n**Example Usage:**\\n\\n```\\nEnter a non-negative integer: 5\\n\\nThe factorial of 5 is 120\\n```\\n\\n**Note:**\\n\\n* The `factorial` function can handle negative numbers, but it returns `0!` for negative values.\\n* The time complexity of calculating the factorial of a number is O(n), where n is the input integer. This is because it involves iterating through the numbers from 2 to n.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\":\"How to write factorial program in python?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eb5b895f-9439-42fd-ae78-a94a5dcd7fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Here\\'s how you can write a factorial program in Python:\\n\\n```python\\n# Define the factorial function\\ndef factorial(n):\\n    # Initialize the factorial to 1\\n    factorial = 1\\n\\n    # Iterate through the numbers from 2 to n\\n    for i in range(2, n + 1):\\n        # Multiply the factorial of i by the factorial of n\\n        factorial *= i\\n\\n    # Return the factorial of n\\n    return factorial\\n\\n\\n# Get the input from the user\\nn = int(input(\"Enter a non-negative integer: \"))\\n\\n# Calculate and print the factorial of n\\nprint(f\"The factorial of {n} is {factorial(n)}\")\\n```\\n\\n**Explanation:**\\n\\n* The `factorial` function takes an integer `n` as input.\\n* It initializes the `factorial` variable to 1, as the factorial of 0 is defined as 1.\\n* It then iterates through the numbers from 2 to `n` and multiplies the factorial of each number by the factorial of `n`.\\n* Finally, it returns the factorial of `n`.\\n* The user is prompted to input a non-negative integer, and the program calculates and prints the factorial of that number using the `factorial` function.\\n\\n**Example Usage:**\\n\\n```\\nEnter a non-negative integer: 5\\n\\nThe factorial of 5 is 120\\n```\\n\\n**Note:**\\n\\n* The `factorial` function can be very computationally expensive for large values of `n`.\\n* You can optimize the program by using a mathematical formula to calculate the factorial of `n` in O(1) time, but this approach may not be suitable for very large values of `n`.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model = \"gemma:2b\")\n",
    "#output = StrOutputParser()\n",
    "(prompt|llm).invoke({\"question\":\"How to write factorial program in python?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d39d72c0-f29b-492e-ad0d-269da8e19e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's how to write a factorial program in Python:\n",
      "\n",
      "```python\n",
      "# Define the factorial function\n",
      "def factorial(n):\n",
      "  \"\"\"\n",
      "  Calculates the factorial of a non-negative integer n.\n",
      "\n",
      "  Args:\n",
      "    n: The non-negative integer for which to calculate the factorial.\n",
      "\n",
      "  Returns:\n",
      "    The factorial of n.\n",
      "  \"\"\"\n",
      "\n",
      "  # Initialize the factorial to 1.\n",
      "  factorial = 1\n",
      "\n",
      "  # Iterate through the numbers from 1 to n.\n",
      "  for i in range(1, n + 1):\n",
      "    # Multiply the factorial of i by the factorial of n.\n",
      "    factorial *= i\n",
      "\n",
      "  # Return the factorial of n.\n",
      "  return factorial\n",
      "\n",
      "\n",
      "# Get the input from the user.\n",
      "n = int(input(\"Enter a non-negative integer: \"))\n",
      "\n",
      "# Calculate and print the factorial of n.\n",
      "result = factorial(n)\n",
      "print(f\"The factorial of {n} is {result}\")\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. The `factorial` function takes one argument, `n`, which is the non-negative integer for which we want to calculate the factorial.\n",
      "2. It initializes the `factorial` variable to 1, as the factorial of 1 is defined as 1.\n",
      "3. It then iterates through the numbers from 1 to `n` (inclusive) using a `for` loop.\n",
      "4. Inside the loop, it multiplies the `factorial` of `i` by the `factorial` of `n`. This accumulates the products of all the numbers from 1 to `n` in the `factorial` variable.\n",
      "5. After the loop completes, it returns the final `factorial` value, which is the factorial of `n`.\n",
      "6. The user is prompted to input a non-negative integer, and the program calculates and prints the factorial of that number using the `factorial` function.\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "```\n",
      "Enter a non-negative integer: 5\n",
      "\n",
      "The factorial of 5 is 120\n",
      "```\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* The factorial of 0 is defined as 1.\n",
      "* This program assumes that the input integer is a valid non-negative integer. If the user enters a non-positive or negative integer, the program will raise an error.\n"
     ]
    }
   ],
   "source": [
    "print((prompt|llm).invoke({\"question\":\"How to write factorial program in python?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "85c8cc64-2cff-4fc0-9f77-8bf1d0ecffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a step-by-step guide on how to write a factorial program in Python:\n",
      "\n",
      "**Step 1: Import the necessary module**\n",
      "\n",
      "```python\n",
      "import math\n",
      "```\n",
      "\n",
      "**Step 2: Define the factorial function**\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculates the factorial of a non-negative integer n.\n",
      "\n",
      "    Args:\n",
      "        n (int): The non-negative integer whose factorial is to be calculated.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "    \"\"\"\n",
      "```\n",
      "\n",
      "**Step 3: Handle base cases**\n",
      "\n",
      "```python\n",
      "if n == 0:\n",
      "    return 1\n",
      "elif n == 1:\n",
      "    return 1\n",
      "```\n",
      "\n",
      "These cases handle the base cases of factorial calculation: 0! = 1! = 1 and 1! = 1!.\n",
      "\n",
      "**Step 4: Calculate the factorial recursively**\n",
      "\n",
      "```python\n",
      "for i in range(2, n + 1):\n",
      "    factorial(i) = factorial(i - 1) * i\n",
      "```\n",
      "\n",
      "**Step 5: Return the factorial**\n",
      "\n",
      "```python\n",
      "return factorial(n)\n",
      "```\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "```python\n",
      "# Calculate the factorial of 5\n",
      "factorial_result = factorial(5)\n",
      "\n",
      "# Print the result\n",
      "print(f\"The factorial of {5} is {factorial_result}\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "The factorial of 5 is 120\n",
      "```\n",
      "\n",
      "**Tips:**\n",
      "\n",
      "* You can use a generator expression to calculate the factorial of a range of numbers.\n",
      "* Use the `math.factorial()` function for built-in functionality.\n",
      "* Consider using a library like `numpy` for more advanced factorial calculations.\n"
     ]
    }
   ],
   "source": [
    "print((prompt|llm|output).invoke({\"question\":\"How to write factorial program in python?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a64f59-1df8-4811-a1a5-5f31a7438acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "use case \n",
    "---------\n",
    "Req1: Home AboutUs  News\n",
    "                     |->City1\n",
    "                     |->City2\n",
    "\n",
    ".... site is live\n",
    "....\n",
    "Req2: Home AboutUs News\n",
    "                     |->City1\n",
    "                     |->City2\n",
    "                     |->City3 <== newInput\n",
    "                     |->..\n",
    "...\n",
    "Req3 : Home About News ContactUs\n",
    "\n",
    "# metaprogramming - Highorder code -> function(another_function)\n",
    "#\n",
    "decorator - design pattern\n",
    "\n",
    "def news(a): <== decorator\n",
    "    def wrapper_code(): <== wrapper\n",
    "        a() \n",
    "    return wrapper_code\n",
    "\n",
    "def city1():\n",
    "    print('<h1>City1 news</h1>')\n",
    "\n",
    "city1=news(city1)\n",
    "city1()\n",
    "\n",
    "@news\n",
    "def city2():\n",
    "    print(\"<h2>City2 news</h2>\")\n",
    "\n",
    "city2() # same as  city2=news(city2) ->city2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9c0e49cf-6d82-4264-98bc-c6263c2e72a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>City1 news</h1>\n"
     ]
    }
   ],
   "source": [
    "def news(a): #<== decorator\n",
    "    def wrapper_code(): #<== wrapper\n",
    "        a() \n",
    "    return wrapper_code\n",
    "\n",
    "def city1():\n",
    "    print('<h1>City1 news</h1>')\n",
    "\n",
    "city1=news(city1)\n",
    "city1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0690675d-fd64-438d-83f9-95169ea10c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@news\n",
    "def city2():\n",
    "    print('<h2>City2 news</h2>')\n",
    "\n",
    "@news\n",
    "def city3():\n",
    "    print('<h3>City3 news</h3>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a19a013-cbe4-419f-9a9d-e1a520750e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>City2 news</h2>\n"
     ]
    }
   ],
   "source": [
    "city2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "906e0c40-6be2-402b-96cc-bd29b48ff953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3>City3 news</h3>\n"
     ]
    }
   ],
   "source": [
    "city3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e5c2cb61-717a-4534-91fa-4d9a009d7a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>City1 news</h1>\n"
     ]
    }
   ],
   "source": [
    "city1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9957827-0c85-46f9-a1c0-b13ae28206ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Building A chatbot\n",
    "# ----------------------\n",
    "# This chatbot will be able to have a conversation and remember previous interactions\n",
    "# \n",
    "# RAG -> Conversational RAG - Enable a chatbot experience over an external data source \n",
    "# Agents -> Build a chatbot that can take actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "074afac5-191e-45c0-9136-b9a36cd4fe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_groq\n",
      "  Using cached langchain_groq-0.2.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
      "  Using cached groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langchain_groq) (0.3.19)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (0.1.143)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (8.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.32.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\theeba\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.2.2)\n",
      "Using cached langchain_groq-0.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached groq-0.12.0-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: groq, langchain_groq\n",
      "Successfully installed groq-0.12.0 langchain_groq-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8654bd19-0d09-46eb-a021-d545c811e254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b7c5b46b-5833-47a5-8874-8d87f383f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3d68c34f-bae2-4b85-97af-cd5c8cfe5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "308e886b-e6c9-415b-8e25-ca252dd67aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001D61659EE10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D616610380>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "72e0025b-3e18-4ca0-9912-0f68185c2346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Karthik,\\n\\nIt's nice to meet you!  \\n\\nI'm happy to help in any way I can.  What can I do for you as an instructor? Perhaps you'd like to:\\n\\n* **Brainstorm lesson ideas**\\n* **Get help with explaining a concept**\\n* **Create quizzes or exercises**\\n* **Find resources for your students**\\n\\n\\nLet me know how I can be of service!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 19, 'total_tokens': 113, 'completion_time': 0.170909091, 'prompt_time': 8.5069e-05, 'queue_time': 0.01461207, 'total_time': 0.17099416}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-6d0a14dc-a185-4921-ad8e-171ae8c335f4-0', usage_metadata={'input_tokens': 19, 'output_tokens': 94, 'total_tokens': 113})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi,My name is karthik and im Instructor\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "169e8894-eeda-40bd-a786-9ce031f36918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hey there! üòä  What can I do for you today, Karthik? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 42, 'total_tokens': 62, 'completion_time': 0.036363636, 'prompt_time': 0.000550389, 'queue_time': 0.014617171, 'total_time': 0.036914025}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-545a66e3-3c00-4903-a57a-c65b0207eacf-0', usage_metadata={'input_tokens': 42, 'output_tokens': 20, 'total_tokens': 62})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke([HumanMessage(content=\"Hi,My name is karthik and im Instructor\"),\n",
    "             AIMessage(content=\"Hello Karthik! it's nice to meet you\"),\n",
    "            HumanMessage(content=\"Hey \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fdc338-ff35-4727-9bc3-df53c392ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory - follow/remeber previous Query ->chain (part of Input)\n",
    "#          - history_session - chain - Runnable\n",
    "# chat_message  - chat_history - runnable(session) (history)\n",
    "#                                      # session - sessionID \n",
    "#                                         |->config - sessionID - chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "95e766a3-c608-4cbe-8d13-3b93d48561b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "41b7137c-192b-4c59-bac6-d4637f2c0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id:str) ->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "578f10cc-15d3-484f-a51a-30afdc1892f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.history.RunnableWithMessageHistory'>\n"
     ]
    }
   ],
   "source": [
    "print(type(with_message_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "07be17c1-eb55-4de9-b2b0-0d41f6b33465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(RunnableWithMessageHistory)\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "87b02948-7a4a-4979-88ab-3f26d1cced38",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Hi,My name is karthik and im Instructor\")],config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "05477ec5-d1e5-4c77-b4d3-fb5234348666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Karthik, it's nice to meet you!\\n\\nBeing an instructor is a rewarding profession. What subjects do you teach?  \\n\\nI'm happy to help you with any questions you might have or even brainstorm ideas for your lessons.\\n\""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c9707865-fc69-4040-9197-91aee4bfc8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Karthik.  \\n\\nI remember that from our first conversation! üòä  Is there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 85, 'total_tokens': 116, 'completion_time': 0.056363636, 'prompt_time': 0.002516811, 'queue_time': 0.013094947999999999, 'total_time': 0.058880447}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-0f15a59b-9d7d-44b0-8676-8f3bd8073ec9-0', usage_metadata={'input_tokens': 85, 'output_tokens': 31, 'total_tokens': 116})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "75a0f750-296f-4881-83e5-76e5e415b6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to remember it!\\n\""
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the config ->session ID\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=config1)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bb77a678-372f-4a5e-80f0-c09a47d809c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Shiv, it's nice to meet you! üëã  \\n\\nIs there anything I can help you with today?\\n\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Hi my name is shiv\")],config=config1)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b4590ad0-40b9-4271-af89-601021a5c732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Shiv!  üòä  \\n\\nI remember you told me earlier. \\n'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Whats my name\")],config=config1)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "49a62737-330b-4577-96db-d3acfa235f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\",\"you are helpful AI assistant\"),\n",
    "                                            MessagesPlaceholder(variable_name=\"messages\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f161da66-fcde-4720-9e21-4bc858d50040",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "985bdc5e-70d2-4d5d-bd62-5e85343874ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Karthik, it's nice to meet you!\\n\\nI'm glad to be your helpful AI assistant. What can I do for you today as an instructor? üòÑ \\n\\nDo you need help with:\\n\\n* **Creating lesson plans or course materials?**\\n* **Finding educational resources?**\\n* **Grading assignments?**\\n* **Communicating with students?**\\n* **Anything else?**\\n\\nJust let me know how I can be of service! üìöüí°\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 24, 'total_tokens': 128, 'completion_time': 0.189090909, 'prompt_time': 0.000154359, 'queue_time': 0.015163932, 'total_time': 0.189245268}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2c3bd7d5-a266-4888-ac5d-0c8806da2a72-0', usage_metadata={'input_tokens': 24, 'output_tokens': 104, 'total_tokens': 128})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi,My name is karthik and im Instructor\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "95451632-5d1d-4464-ab28-a98b433623d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4c67993e-01f5-4e9c-a0e4-a8341ffd34f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a helpful AI assistant, I don't have access to any personal information about you, including your name. If you'd like to tell me your name, I'd be happy to use it! üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 19, 'total_tokens': 68, 'completion_time': 0.089090909, 'prompt_time': 7.652e-05, 'queue_time': 0.013424848999999999, 'total_time': 0.089167429}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f4bb4eda-fbe7-4c53-a241-a27fa5fc9f4c-0', usage_metadata={'input_tokens': 19, 'output_tokens': 49, 'total_tokens': 68})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the config ->session ID\n",
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = with_message_history.invoke([HumanMessage(content=\"what's my name\")],config=config)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5f7343fe-24ad-483e-9a25-591e0a4d68a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Leo, it's nice to meet you! üëã \\n\\nWhat can I do to help you today? üòä  \\n\\n\""
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0df0b76c-4caf-4856-b2a3-384fe9d6fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",\"you are helpful AI assist,Answer all the questsions in {language}.\",),\n",
    "     MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5c6919cc-f4d5-461b-8e86-22162fea36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "31f86c86-c5c6-4efe-8716-39dfd8f30475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§≤‡§ø‡§Ø‡•ã! ‡§Æ‡•Å‡§ù‡•á ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§ \\n\\n‡§Ü‡§™‡§ï‡•á ‡§ï‡•ã‡§à ‡§∏‡§µ‡§æ‡§≤ ‡§π‡•à‡§Ç ‡§§‡•ã ‡§™‡•Ç‡§õ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§Ç‡§ï‡•ã‡§ö ‡§® ‡§ï‡§∞‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•Ç‡§Ç‡§ó‡§æ ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡•Ç‡§Å‡•§ üòä  \\n\\n'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fe407963-5392-4b49-ab33-62fcb2763ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç, Leo! \\n\\n‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æâ‡Æ§‡Æµ‡ØÅ‡Æµ‡Æ§‡Æ±‡Øç‡Æï‡Ææ‡Æï ‡Æá‡Æô‡Øç‡Æï‡Øá ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. ‡Æé‡Æ®‡Øç‡Æ§ ‡Æï‡Øá‡Æ≥‡Øç‡Æµ‡Æø‡Æï‡Æ≥‡ØÅ‡ÆÆ‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡Ææ‡Æ≤‡Øç, ‡Æ®‡Æø‡Æö‡Øç‡Æö‡ÆØ‡ÆÆ‡Øç ‡Æï‡Øá‡Æ≥‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç. üòÄ \\n'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"Tamil\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f2ad9d2e-f0e2-442a-abcc-4e3fb84ab631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡≤®‡≤Æ‡≤∏‡≥ç‡≤ï‡≤æ‡≤∞ ‡≤≤‡≤ø‡≤Ø‡≥ã! üòä\\n\\n‡≤®‡≤®‡≥ç‡≤® ‡≤π‡≥Ü‡≤∏‡≤∞‡≥Å ‡≤¨‡≤æ‡≤∞‡≥ç‡≤¨‡≤æ. ‡≤®‡≤æ‡≤®‡≥Å ‡≤®‡≤ø‡≤Æ‡≤ó‡≥Ü ‡≤∏‡≤π‡≤ï‡≤∞‡≤ø‡≤∏‡≤≤‡≥Å ‡≤∏‡≤ø‡≤¶‡≥ç‡≤ß‡≤µ‡≤æ‡≤ó‡≤ø‡≤∞‡≥Å‡≤µ ‡≤∏‡§π‡§æ‡§Ø‡≤ï AI ‡≤Ü‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥á‡≤®‡≥Ü. \\n\\n‡≤®‡≤ø‡≤Æ‡≤ó‡≥Ü ‡≤Ø‡≤æ‡≤µ ‡≤™‡≥ç‡≤∞‡≤∂‡≥ç‡≤®‡≥Ü‡≤ó‡≤≥‡≤ø‡≤¶‡≥ç‡≤¶‡≤æ‡≤µ‡≥Ü?  üòä\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"kannada\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "715106c8-cbac-4405-b7bc-9a53e46299be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞Æ‡∞∏‡±ç‡∞§‡±á ‡∞≤‡±Ä‡∞Ø‡±ã! üòä \\n\\n‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç‡∞ó‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å. ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞è‡∞µ‡±à‡∞®‡∞æ ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡±Å ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞æ? üòä\\n'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"telugu\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "eb5f4f10-fb26-475f-aa06-e02d25f90f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour Leo !\\n\\nEnchant√© de te rencontrer.  Comment puis-je t'aider aujourd'hui ? N'h√©site pas √† me poser toutes tes questions. üòä \\n\""
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"french\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3af2969f-154e-42b1-be59-e6dbf9a55f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞, ‡§≤‡•Ä‡§ì! ‡§Æ‡•Ä ‡§§‡•Å‡§Æ‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§¶‡§§‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§Ø‡•á‡§•‡•á ‡§Ü‡§π‡•á. ‡§§‡•Å‡§Æ‡§ö‡•ç‡§Ø‡§æ ‡§ï‡•ã‡§£‡§§‡•ç‡§Ø‡§æ‡§π‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§Æ‡§≤‡§æ ‡§µ‡§ø‡§ö‡§æ‡§∞‡•Ç ‡§®‡§ï‡§æ.üòä \\n'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hello my name is leo\")],\"language\":\"marati\"})\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
