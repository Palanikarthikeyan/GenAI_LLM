{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66ddda-669b-4284-8233-38a5ae16b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "python \n",
    " - general purpose programming \n",
    " - opensource\n",
    " - python.org => python download -> cpython - we can do python programming \n",
    "                 |-> ML libs - numpy pandas matplotlib scikitlearn scipy .. not avaiable by default\n",
    "                 |-> pip install numpy (or) Linux/mac  pip3 install numpy\n",
    "\n",
    "    Vs\n",
    " - anaconda portal\n",
    "                |-> ipython\n",
    "                |-> by default numpy pandas matplotlib scikitlean scipy  modules are available \n",
    "\n",
    "     python programming model\n",
    "         |->class <--> object\n",
    "\n",
    "     In python everything is an object\n",
    "                                 |->class - type \n",
    "\n",
    "     i = 10 \n",
    "          |-> the value 10 is an object - it's belongs to int class/type \n",
    "          |-> each object having own memory \n",
    "          |-> by default in Cpython - GlobalInterpreterLock (GIL) is enabled \n",
    "                          Vs\n",
    "                    ipython - GIL is disabled                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d788c7-d79c-4430-92b0-4dddcf7186ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7702fb0a-29e1-4c9e-ac79-9d7519a05053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68130ee2-cd0f-44ad-bb15-88499558b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello student welcome to learn NLP lecture\n",
    "Please do the activity for entire NLP course\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76492ac3-ccf6-4a37-969c-39c0f82f6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.tokenize.sent_tokenize\n",
    "|     |          |->function\n",
    "|    module - python file(.py)\n",
    "projectName(Folder/directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f1c15c-9dd7-4302-8214-83b50333ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7547845f-ff9e-443c-a361-f7e4365e02e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello student welcome to learn NLP lecture\\nPlease do the activity for entire NLP course']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7c35f-88ee-4f98-b5bc-fd7272530e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') # nltk.downalod('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18e1ac4-bad4-4b71-8d35-2469d06ee33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n"
     ]
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "print(type(documents),len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f6acb7-1874-4f06-858c-7771c6ae532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71ebd745-9bb3-4b53-af1a-90ceb9587bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'student',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " 'lecture',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'the',\n",
       " 'activity',\n",
       " 'for',\n",
       " 'entire',\n",
       " 'NLP',\n",
       " 'course']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a9a1ce-9f1b-4e66-8c4e-905c5dac3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "print(type(words),len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ca36fc8-a11d-42eb-8b82-6b638a616b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Hello UserA,How are you doing? OK. I am doing good!:-)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5564496a-fca5-4b7d-a86a-2502de5bae0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello UserA,How are you doing?', 'OK.', 'I am doing good!', ':-)']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4849b5f1-1847-49b2-b691-4b7f089f1c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43fbf5d9-15da-4abc-a5e2-85989fac19b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'UserA',\n",
       " ',',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'OK',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'good',\n",
       " '!',\n",
       " ':',\n",
       " '-',\n",
       " ')']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad66ceb1-c59f-4980-b946-604462b733ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d527673-b945-4e40-989c-8a24b45ab5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'UserA',\n",
       " ',',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'OK',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'good',\n",
       " '!:-)']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51c359d1-45f8-49c3-b29a-751537058793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a632d55-b2a0-44b2-a57e-439e353ab432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.treebank.TreebankWordTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(TreebankWordTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "debd7a81-01cc-4c74-ab5c-909fe1574a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'UserA',\n",
       " ',',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'OK.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'doing',\n",
       " 'good',\n",
       " '!',\n",
       " ':',\n",
       " '-',\n",
       " ')']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d54dfe86-3c44-44c3-b882-201a263ee1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"writing\",\"writes\",\"eating\",\"eats\",\"programming\",\"programs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f83d416-7221-482e-aeb0-88648f2f7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8eb91d0-6209-4131-9b40-ab7ea649be50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.stem.porter.PorterStemmer'>\n"
     ]
    }
   ],
   "source": [
    "print(PorterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f43c39e4-78b7-42a4-b5b4-cc8a059812f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3922ddac-44fb-419b-9154-4144a6fc9741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e155a2f7-9e12-4626-840a-8a60892d84af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing----->write\n",
      "writes----->write\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "programming----->program\n",
      "programs----->program\n"
     ]
    }
   ],
   "source": [
    "for var in words:\n",
    "    print(var+'----->'+stemming.stem(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6bb679c-9dc7-4ba2-84cf-f843a4a51d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a966625-6444-4a67-ae17-2b2eba5c43a2",
   "metadata": {},
   "source": [
    "# RegexpStemmer class\n",
    "#\n",
    "# Regular Expression - pattern \n",
    "# --------------\n",
    "import re\n",
    "re.findall('ing','programmingabcingcodeing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed0cd7-ba33-4e9a-be6a-ee2153228c06",
   "metadata": {},
   "source": [
    "re.findall('ing$','programmingabcingcodeing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b414f38a-f5f0-4a94-8fb9-87d92e0dd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpStemmer class\n",
    "#------------------------\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9478d1a-5336-4745-9fcf-fdba5c5b0890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RegexpStemmer: 'ing$|s$|e$|able$'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegexpStemmer('ing$|s$|e$|able$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "899edfc4-e550-41c3-b121-dfab250937c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$')\n",
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b5bb36d-31ea-4edf-abce-6a129bd43e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd154db2-584a-4fc6-b44a-0e4114968506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'),stemming.stem('sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15370f3b-9079-438d-84d1-9aa305047fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "##--------------\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "629f007a-7391-4266-8cd7-4872f64d8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\theeba\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b944046-f6f6-47d8-ae36-bb21519e9d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f2182b9-09db-407c-af57-0864d5d600a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmatize in module nltk.stem.wordnet:\n",
      "\n",
      "lemmatize(word: str, pos: str = 'n') -> str method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
      "    Lemmatize `word` using WordNet's built-in morphy function.\n",
      "    Returns the input word unchanged if it cannot be found in WordNet.\n",
      "\n",
      "    :param word: The input word to lemmatize.\n",
      "    :type word: str\n",
      "    :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
      "        `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
      "        for satellite adjectives.\n",
      "    :param pos: str\n",
      "    :return: The lemma of `word`, for the given `pos`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lemmatizer.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d4411e7-4f3a-4c4b-b455-d71ebd9270b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c92f1a95-a966-47d8-986f-856c7726fb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "61d69244-4d28-4228-b0d1-1789e85b33d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136dee03-08e7-4556-9fd5-c6ff576e9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos - part of speech tag\n",
    "n = noun\n",
    "v = verb\n",
    "a = adjectives\n",
    "r = adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ccc4465d-e28d-4226-8f01-7ec3aa7605e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"writing\",\"writes\",\"eating\",\"eats\",\"programming\",\"programs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "353c3582-9ddc-4df6-b03e-0ffc76dfd08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing----->writing\n",
      "writes----->writes\n",
      "eating----->eating\n",
      "eats----->eats\n",
      "programming----->programming\n",
      "programs----->program\n"
     ]
    }
   ],
   "source": [
    "for var in words:\n",
    "    print(var+'----->'+lemmatizer.lemmatize(var,pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6d4b98f5-0fb9-48cc-ab3c-5d99509e4d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing----->write\n",
      "writes----->write\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "programming----->program\n",
      "programs----->program\n"
     ]
    }
   ],
   "source": [
    "for var in words:\n",
    "    print(var+'----->'+lemmatizer.lemmatize(var,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89f8e0f5-d0f5-44c5-a1f0-befedfa4f5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'),stemming.stem('sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55998924-4181-433a-99c0-7cb8ff494126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('fairly'),lemmatizer.lemmatize('sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce9b91e3-603f-40e8-8187-b9d5e55a164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To list of stopwords \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "73ef70d9-3981-4e6e-acd9-ff62964afc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\theeba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9a488d54-874c-4022-9af1-e0e2bebce4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8b936167-3896-4ec1-a770-420b8e1b64cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8ce083a3-6bb9-4440-b767-069a7c4eff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acitivity - 1\n",
    "paragraph = '''This report offers perspectives and practical guidelines to the Cornell community, specifically on the use of Generative Artificial Intelligence (GenAI) in the practice and dissemination of academic research. As emphasized in the charge to a Cornell task force representing input across all campuses, the report aims to establish the initial set of perspectives and cultural norms for Cornell researchers, research team leaders, and research administration staff. It is meant as internal advice rather than a set of binding rules. As GenAI policies and guardrails are rapidly evolving, we stress the importance of staying current with the latest developments, and updating procedures and rules governing the use of GenAI tools in research thoughtfully over time. This report was developed within the same 12-month period that GenAI became available to a much wider number of researchers (and citizens) than AI specialists who help create such tools. While the Cornell community is the intended audience, this report is publicly available as a resource for other research communities to use or adapt. No endorsement of specific tools is implied, but specific examples are referenced to illustrate concepts.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ded2d836-1d52-441a-a5a9-8579d6133bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfc9f6-048d-44f9-81e7-e8b9f2fad4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "#nltk.dowloads('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9e722df1-3bc3-43e6-a85e-5416fd424d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a709c292-f8fc-4f7d-991f-8d7ef625a7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This report offers perspectives and practical guidelines to the Cornell community, specifically on the use of Generative Artificial Intelligence (GenAI) in the practice and dissemination of academic research.',\n",
       " 'As emphasized in the charge to a Cornell task force representing input across all campuses, the report aims to establish the initial set of perspectives and cultural norms for Cornell researchers, research team leaders, and research administration staff.',\n",
       " 'It is meant as internal advice rather than a set of binding rules.',\n",
       " 'As GenAI policies and guardrails are rapidly evolving, we stress the importance of staying current with the latest developments, and updating procedures and rules governing the use of GenAI tools in research thoughtfully over time.',\n",
       " 'This report was developed within the same 12-month period that GenAI became available to a much wider number of researchers (and citizens) than AI specialists who help create such tools.',\n",
       " 'While the Cornell community is the intended audience, this report is publicly available as a resource for other research communities to use or adapt.',\n",
       " 'No endorsement of specific tools is implied, but specific examples are referenced to illustrate concepts.']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(paragraph) \n",
    "# This is Paragraph - divided into sentence - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5d6225a6-93e6-4820-9a3d-a052272afeb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 7\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph) \n",
    "print(type(sentences),len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bba26-950b-450b-9089-01877c9b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply stopwords and filter and then apply steamming\n",
    "### ....................................................\n",
    "stopwords.words('english') ->collection  - we can iterate one by one\n",
    "      ->stemmer.stem(<collectionInput>)\n",
    "      ->Append to list\n",
    "      ->We can combine muliple items to single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "be283f45-2b53-45e2-b298-b653af3bf4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer =PorterStemmer() # stemmer object from PorterStemmer class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3c958a63-9439-46fc-b5b3-dd29e8d02090",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5d9d828f-dcf2-43d6-a984-abea6e154109",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in stopwords.words('english'):\n",
    "    r = stemmer.stem(var)\n",
    "    L.append(r) \n",
    "# To get list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bfa4cfb8-d7ba-40d8-9a95-fb06807b6aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L) # total no.of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "007db30e-0138-4076-be64-c2c5aed3a6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110, 120, 130, 140, 150]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var+ 100 for var in [10,20,30,40,50]] \n",
    "# list comprehesion - like list append operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1dd3fdfd-e001-462e-bd78-f858fd2d3a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords filter it then apply stemming\n",
    "for var in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[var])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[var]=' '.join(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8108a826-0f21-4880-ab2c-eeb12aa1c6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi report offer perspect practic guidelin cornel commun , specif use gener artifici intellig ( genai ) practic dissemin academ research .',\n",
       " 'as emphas charg cornel task forc repres input across campus , report aim establish initi set perspect cultur norm cornel research , research team leader , research administr staff .',\n",
       " 'it meant intern advic rather set bind rule .',\n",
       " 'as genai polici guardrail rapidli evolv , stress import stay current latest develop , updat procedur rule govern use genai tool research thought time .',\n",
       " 'thi report develop within 12-month period genai becam avail much wider number research ( citizen ) ai specialist help creat tool .',\n",
       " 'while cornel commun intend audienc , report publicli avail resourc research commun use adapt .',\n",
       " 'no endors specif tool impli , specif exampl referenc illustr concept .']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences # this not showing good. -> we can go to lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6f162fb7-3c69-49b8-904e-81a2472bc218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This report offer perspective practical guideline Cornell community , specifically use Generative Artificial Intelligence ( GenAI ) practice dissemination academic research .', 'As emphasized charge Cornell task force representing input across campus , report aim establish initial set perspective cultural norm Cornell researcher , research team leader , research administration staff .', 'It meant internal advice rather set binding rule .', 'As GenAI policy guardrail rapidly evolving , stress importance staying current latest development , updating procedure rule governing use GenAI tool research thoughtfully time .', 'This report developed within 12-month period GenAI became available much wider number researcher ( citizen ) AI specialist help create tool .', 'While Cornell community intended audience , report publicly available resource research community use adapt .', 'No endorsement specific tool implied , specific example referenced illustrate concept .']\n"
     ]
    }
   ],
   "source": [
    "## Activity-2 \n",
    "## Apply  - lemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # create lemmatizer instance\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Apply stopwords filter it then apply stemming\n",
    "for var in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[var])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[var]=' '.join(words)    \n",
    "\n",
    "print(sentences) # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "001750a7-a898-4761-b0ea-763798186bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This report offer perspective practical guideline Cornell community , specifically use Generative Artificial Intelligence ( GenAI ) practice dissemination academic research .', 'As emphasize charge Cornell task force represent input across campus , report aim establish initial set perspective cultural norm Cornell researcher , research team leader , research administration staff .', 'It mean internal advice rather set bind rule .', 'As GenAI policy guardrail rapidly evolve , stress importance stay current latest development , update procedure rule govern use GenAI tool research thoughtfully time .', 'This report develop within 12-month period GenAI become available much wider number researcher ( citizen ) AI specialist help create tool .', 'While Cornell community intend audience , report publicly available resource research community use adapt .', 'No endorsement specific tool imply , specific example reference illustrate concept .']\n"
     ]
    }
   ],
   "source": [
    "# Apply stopwords filter it then apply stemming\n",
    "for var in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[var])\n",
    "    words = [lemmatizer.lemmatize(word,pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[var]=' '.join(words)    \n",
    "\n",
    "print(sentences) # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9aeed3e7-cddd-4148-bea8-956eb83dce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44a45c-9c20-4b0b-a3b9-831066177776",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data_File')\n",
    "pd.read_csv('data_File',sep='\\t')\n",
    "pd.read_csv('data_File',sep='\\t',names=['Field1','Field2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1a399d77-cdcb-4ebb-8094-5c9b74dbd7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4c23335a-fa52-4f8d-8b0a-6cfdfc047582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b631dd5e-103c-4c7c-8c88-5505395942b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c3eccf2c-3d7e-44db-a19f-2bdf1e15995a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      0\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "886    0\n",
       "887    0\n",
       "888    1\n",
       "889    0\n",
       "890    0\n",
       "Name: SibSp, Length: 891, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SibSp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5ef426e5-2b55-4116-9862-197f4c823b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('spam.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f804f3-b469-45b4-ad1e-32119ea842a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam',names=['label','messages'])\n",
    "df\n",
    "|                        |\n",
    "label                  message\n",
    "ham                     Go until ....\n",
    "spam                    free entry in 2k\n",
    "ham                       text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e7f8a479-6c1d-4138-bc5c-48ff3b238d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fad5eef7-2cfb-48c3-bc9f-baa5d6f22fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # information about dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ff1c53bd-43de-471d-82fa-874164a23ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 5)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e40251ec-be90-426f-af2d-b6cdf03863df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c42151ff-c188-46d4-8ece-09b6b68f9a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9be15ac9-33a6-4c3f-97a2-05314cdfa8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thatåÕs the way u feel. ThatåÕs the wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      v1                                                 v2 Unnamed: 2  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1    ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3    ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "5   spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
       "6    ham  Even my brother is not like to speak with me. ...        NaN   \n",
       "7    ham  As per your request 'Melle Melle (Oru Minnamin...        NaN   \n",
       "8   spam  WINNER!! As a valued network customer you have...        NaN   \n",
       "9   spam  Had your mobile 11 months or more? U R entitle...        NaN   \n",
       "10   ham  I'm gonna be home soon and i don't want to tal...        NaN   \n",
       "11  spam  SIX chances to win CASH! From 100 to 20,000 po...        NaN   \n",
       "12  spam  URGENT! You have won a 1 week FREE membership ...        NaN   \n",
       "13   ham  I've been searching for the right words to tha...        NaN   \n",
       "14   ham                I HAVE A DATE ON SUNDAY WITH WILL!!        NaN   \n",
       "15  spam  XXXMobileMovieClub: To use your credit, click ...        NaN   \n",
       "16   ham                         Oh k...i'm watching here:)        NaN   \n",
       "17   ham  Eh u remember how 2 spell his name... Yes i di...        NaN   \n",
       "18   ham  Fine if thatåÕs the way u feel. ThatåÕs the wa...        NaN   \n",
       "19  spam  England v Macedonia - dont miss the goals/team...        NaN   \n",
       "\n",
       "   Unnamed: 3 Unnamed: 4  \n",
       "0         NaN        NaN  \n",
       "1         NaN        NaN  \n",
       "2         NaN        NaN  \n",
       "3         NaN        NaN  \n",
       "4         NaN        NaN  \n",
       "5         NaN        NaN  \n",
       "6         NaN        NaN  \n",
       "7         NaN        NaN  \n",
       "8         NaN        NaN  \n",
       "9         NaN        NaN  \n",
       "10        NaN        NaN  \n",
       "11        NaN        NaN  \n",
       "12        NaN        NaN  \n",
       "13        NaN        NaN  \n",
       "14        NaN        NaN  \n",
       "15        NaN        NaN  \n",
       "16        NaN        NaN  \n",
       "17        NaN        NaN  \n",
       "18        NaN        NaN  \n",
       "19        NaN        NaN  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv',encoding='ISO-8859-1')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c1607352-dfe7-470b-9a09-85d27534f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3e3e4a6f-efd3-4b67-b2f5-5927bd2efae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c3f7b129-87fe-4a76-b553-ab929c3eac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning and Preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps_obj = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d2c87-8f97-46c8-a584-f6ca391fb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ->omit special chars - apply regx\n",
    "# ->lowercase chars    - string method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "49cc24b9-9726-432f-abbe-9f3c9e5783a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=\"Hello This (AB) is dev m/c server @234,23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "267b5a00-f726-4284-8442-a6428a12fb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\theeba\\AppData\\Local\\Temp\\ipykernel_14828\\3165996715.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  re.findall('[^\\w\\s]',msg) # same as re.findall('[^a-zA-Z0-9]',msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['(', ')', '/', '@', ',']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[^\\w\\s]',msg) # same as re.findall('[^a-zA-Z0-9]',msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3b9d594b-8b18-450d-a002-e5fad6a8b890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'101,raj,prod,pune'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.sub('oldPattern','replace','inputString')\n",
    "re.sub('sales','prod','101,raj,sales,pune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e1384472-d1ef-48f2-b9b3-07674b761441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'101,raj,sales,pune'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('QA','prod','101,raj,sales,pune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ab021bd4-3b63-4a50-adfa-257480ec60d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello This (AB) is dev m/c server @234,23'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c4b76031-d51e-47fb-ab18-033ae5b98cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello This  AB  is dev m c server  234 23'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^a-zA-Z0-9]',' ',msg) # replace all the specialchars ->space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a9f695ce-9509-4ff9-8b6d-60a3010fc1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello this  ab  is dev m c server  234 23'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^a-zA-Z0-9]',' ',msg).lower() # covert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9f33d967-38c1-4ce2-9f45-2c7b653cd08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'this', 'ab', 'is', 'dev', 'm', 'c', 'server', '234', '23']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^a-zA-Z0-9]',' ',msg).lower().split() # convert from str ->list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c804a2c0-2b54-4b36-8452-3c6755fbbabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[] # empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8bfe96d2-1f5c-4811-be95-04382bb6a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "032549f3-a63d-402e-a4e1-5921c6bcf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(new_df)):\n",
    "    r = re.sub('[^a-zA-Z0-9]',' ',new_df['v1'][i])\n",
    "    r = r.lower()\n",
    "    r = r.split()\n",
    "    r = [ps_obj.stem(word) for word in r if not word in stopwords.words('english')]\n",
    "    r = ' '.join(r)\n",
    "    corpus.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "5232a452-2eec-4219-995d-5f4624450b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "11bd49eb-6b19-4064-a491-f3f24eb2b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(new_df)):\n",
    "    r = re.sub('[^a-zA-Z0-9]',' ',new_df['v2'][i])\n",
    "    r = r.lower()\n",
    "    r = r.split()\n",
    "    r = [ps_obj.stem(word) for word in r if not word in stopwords.words('english')]\n",
    "    r = ' '.join(r)\n",
    "    corpus.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1d43830f-53bd-4f34-8ce7-219aeb09a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though',\n",
       " 'freemsg hey darl 3 week word back like fun still tb ok xxx std chg send 1 50 rcv',\n",
       " 'even brother like speak treat like aid patent',\n",
       " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend callertun',\n",
       " 'winner valu network custom select receivea 900 prize reward claim call 09061701461 claim code kl341 valid 12 hour',\n",
       " 'mobil 11 month u r entitl updat latest colour mobil camera free call mobil updat co free 08002986030',\n",
       " 'gonna home soon want talk stuff anymor tonight k cri enough today',\n",
       " 'six chanc win cash 100 20 000 pound txt csh11 send 87575 cost 150p day 6day 16 tsandc appli repli hl 4 info',\n",
       " 'urgent 1 week free membership 100 000 prize jackpot txt word claim 81010 c www dbuk net lccltd pobox 4403ldnw1a7rw18',\n",
       " 'search right word thank breather promis wont take help grant fulfil promis wonder bless time',\n",
       " 'date sunday',\n",
       " 'xxxmobilemovieclub use credit click wap link next txt messag click http wap xxxmobilemovieclub com n qjkgighjjgcbl',\n",
       " 'oh k watch',\n",
       " 'eh u rememb 2 spell name ye v naughti make v wet',\n",
       " 'fine way u feel way gota b',\n",
       " 'england v macedonia dont miss goal team news txt ur nation team 87077 eg england 87077 tri wale scotland 4txt 1 20 poboxox36504w45wq 16']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "090d18ee-cb3b-4943-b140-248f55c08c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag of words (BOW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=10,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "240e08f9-c47d-42ad-b6bf-b1c367d85fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2c0f395a-0271-4373-8224-e17650260ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ham': 4,\n",
       " 'spam': 6,\n",
       " 'free': 3,\n",
       " 'std': 7,\n",
       " 'txt': 8,\n",
       " 'word': 9,\n",
       " 'prize': 5,\n",
       " 'claim': 2,\n",
       " 'call': 1,\n",
       " '000': 0}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "60fc027b-b0a9-4d9a-a1ce-0110df3a27e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'ham',\n",
       " 'spam',\n",
       " 'go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though',\n",
       " 'freemsg hey darl 3 week word back like fun still tb ok xxx std chg send 1 50 rcv',\n",
       " 'even brother like speak treat like aid patent',\n",
       " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend callertun',\n",
       " 'winner valu network custom select receivea 900 prize reward claim call 09061701461 claim code kl341 valid 12 hour',\n",
       " 'mobil 11 month u r entitl updat latest colour mobil camera free call mobil updat co free 08002986030',\n",
       " 'gonna home soon want talk stuff anymor tonight k cri enough today',\n",
       " 'six chanc win cash 100 20 000 pound txt csh11 send 87575 cost 150p day 6day 16 tsandc appli repli hl 4 info',\n",
       " 'urgent 1 week free membership 100 000 prize jackpot txt word claim 81010 c www dbuk net lccltd pobox 4403ldnw1a7rw18',\n",
       " 'search right word thank breather promis wont take help grant fulfil promis wonder bless time',\n",
       " 'date sunday',\n",
       " 'xxxmobilemovieclub use credit click wap link next txt messag click http wap xxxmobilemovieclub com n qjkgighjjgcbl',\n",
       " 'oh k watch',\n",
       " 'eh u rememb 2 spell name ye v naughti make v wet',\n",
       " 'fine way u feel way gota b',\n",
       " 'england v macedonia dont miss goal team news txt ur nation team 87077 eg england 87077 tri wale scotland 4txt 1 20 poboxox36504w45wq 16']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4fe8fb0a-4d6f-46aa-a0bd-463263b60823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 210)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "a = cv.fit_transform(corpus).toarray()\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e1e212bc-28f6-4cd7-9732-65cb0b889540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |\n",
      " |  Convert a collection of text documents to a matrix of token counts.\n",
      " |\n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |\n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |\n",
      " |  For an efficiency comparison of the different feature extractors, see\n",
      " |  :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : {'filename', 'file', 'content'}, default='content'\n",
      " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
      " |        expected to be a list of filenames that need reading to fetch\n",
      " |        the raw content to analyze.\n",
      " |\n",
      " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      " |        object) that is called to fetch the bytes in memory.\n",
      " |\n",
      " |      - If `'content'`, the input is expected to be a sequence of items that\n",
      " |        can be of type string or byte.\n",
      " |\n",
      " |  encoding : str, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |\n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |\n",
      " |  strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      a direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) means no character normalization is performed.\n",
      " |\n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |\n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |\n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer`` is not callable.\n",
      " |\n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |\n",
      " |  stop_words : {'english'}, list, default=None\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |\n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |\n",
      " |      If None, no stop words will be used. In this case, setting `max_df`\n",
      " |      to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
      " |      and filter stop words based on intra corpus document frequency of terms.\n",
      " |\n",
      " |  token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |\n",
      " |      If there is a capturing group in token_pattern then the\n",
      " |      captured group content, not the entire match, becomes the token.\n",
      " |      At most one capturing group is permitted.\n",
      " |\n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer`` is not callable.\n",
      " |\n",
      " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |\n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |\n",
      " |      .. versionchanged:: 0.21\n",
      " |\n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |\n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      `max_features` ordered by term frequency across the corpus.\n",
      " |      Otherwise, all features are used.\n",
      " |\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |\n",
      " |  binary : bool, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |\n",
      " |  dtype : dtype, default=np.int64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |\n",
      " |  fixed_vocabulary_ : bool\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user.\n",
      " |\n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |\n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |\n",
      " |      This is only available if no vocabulary was given.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer : Convert a collection of text documents to a\n",
      " |      matrix of token counts.\n",
      " |\n",
      " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
      " |      of TF-IDF features.\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> vectorizer.get_feature_names_out()\n",
      " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      " |         'this'], ...)\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> vectorizer2.get_feature_names_out()\n",
      " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |         'this document', 'this is', 'this the'], ...)\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted vectorizer.\n",
      " |\n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return document-term matrix.\n",
      " |\n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |\n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |\n",
      " |  set_fit_request(self: sklearn.feature_extraction.text.CountVectorizer, *, raw_documents: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.feature_extraction.text.CountVectorizer from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |\n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      The options for each parameter are:\n",
      " |\n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |\n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |\n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |\n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |\n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``raw_documents`` parameter in ``fit``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |\n",
      " |  set_transform_request(self: sklearn.feature_extraction.text.CountVectorizer, *, raw_documents: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.feature_extraction.text.CountVectorizer from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``transform`` method.\n",
      " |\n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      The options for each parameter are:\n",
      " |\n",
      " |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
      " |\n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
      " |\n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |\n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |\n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``raw_documents`` parameter in ``transform``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |\n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |\n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |\n",
      " |  build_analyzer(self)\n",
      " |      Return a callable to process input data.\n",
      " |\n",
      " |      The callable handles preprocessing, tokenization, and n-grams generation.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |\n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |\n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |\n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |\n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : bytes or str\n",
      " |          The string to decode.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |\n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "96671985-12d6-460a-aec4-3e7964a5e62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(CountVectorizer)\n",
    "CountVectorizer(max_features=5,binary=True,ngram_range=(2,3)).fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9ec975c5-b558-437d-a1de-3ceb664f2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=10,binary=True,ngram_range=(1,2))\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "c3dd7bc4-9ffa-442b-b90b-6fa92258acfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ham': 3,\n",
       " 'spam': 5,\n",
       " 'free': 2,\n",
       " 'win': 8,\n",
       " 'txt': 6,\n",
       " 'week': 7,\n",
       " 'word': 9,\n",
       " 'prize': 4,\n",
       " '20': 1,\n",
       " '000': 0}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "49db02e9-9432-4d24-ba22-486d77ecb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=10,binary=True,ngram_range=(2,2)) # bigram\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "4b3bf530-e60c-4e1c-80c8-0d704f3ba4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'point crazi': 3,\n",
       " 'press copi': 5,\n",
       " 'prize reward': 7,\n",
       " '000 pound': 0,\n",
       " 'pound txt': 4,\n",
       " 'prize jackpot': 6,\n",
       " 'pobox 4403ldnw1a7rw18': 1,\n",
       " 'promis wont': 9,\n",
       " 'promis wonder': 8,\n",
       " 'poboxox36504w45wq 16': 2}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
